# -*- coding: utf-8 -*-
"""Timeseries

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xGNPxN3pDsuJaMl_YixMcVBxbMcdxj26
"""

# ================================================================
#          ADVANCED TIME SERIES FORECASTING — FULL PROJECT CODE
#             LSTM BASELINE + SEQ2SEQ ATTENTION MODEL
# ================================================================

# ========================
# Install Dependencies
# ========================
!pip install -q numpy pandas matplotlib seaborn scikit-learn torch tqdm

# ========================
# Imports
# ========================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Global parameters
INPUT_LEN = 96
HORIZON = 24
BATCH_SIZE = 64

# ================================================================
# 1. DATA GENERATION
# ================================================================
def generate_multivariate(n_points=6000, n_features=5, seed=0):
    rng = np.random.RandomState(seed)
    t = np.arange(n_points)
    data = []

    for f in range(n_features):
        freq = rng.uniform(0.0005, 0.01)
        phase = rng.uniform(0, 2*np.pi)
        seasonal = np.sin(2*np.pi*freq*t + phase)
        trend = 0.0002 * t * rng.uniform(-1, 1)
        noise = 0.2 * rng.normal(size=n_points)

        coupling = 0.0
        if f > 0:
            coupling = 0.1 * (np.roll(data[-1], 1) - np.mean(data[-1]))

        series = (1 + 0.5*f) * seasonal + trend + noise + coupling

        spikes = (rng.rand(n_points) < 0.001).astype(float) * rng.normal(0, 5, size=n_points)
        series += spikes

        data.append(series)

    return np.vstack(data).T

# Generate & save dataset
arr = generate_multivariate()
df = pd.DataFrame(arr, columns=[f"f{i}" for i in range(arr.shape[1])])
df.to_csv("generated_timeseries.csv", index=False)
print("Dataset saved: generated_timeseries.csv")

# ================================================================
# 2. PREPROCESSING + WINDOW CREATION
# ================================================================
data = df.values
scaler = StandardScaler()
scaled = scaler.fit_transform(data)

X, Y = [], []
for i in range(len(scaled) - INPUT_LEN - HORIZON):
    X.append(scaled[i:i+INPUT_LEN])
    Y.append(scaled[i+INPUT_LEN:i+INPUT_LEN+HORIZON])

X = np.array(X)
Y = np.array(Y)

tensor_X = torch.tensor(X, dtype=torch.float32)
tensor_Y = torch.tensor(Y, dtype=torch.float32)

dataset = torch.utils.data.TensorDataset(tensor_X, tensor_Y)
loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

n_features = data.shape[1]

# ================================================================
# 3. BASELINE LSTM MODEL
# ================================================================
class LSTMBaseline(nn.Module):
    def __init__(self, n_features, hidden=128):
        super().__init__()
        self.lstm = nn.LSTM(n_features, hidden, batch_first=True)
        self.fc = nn.Linear(hidden, n_features * HORIZON)
        self.n_features = n_features

    def forward(self, x):
        out, _ = self.lstm(x)
        last = out[:, -1, :]
        out = self.fc(last)
        return out.view(-1, HORIZON, self.n_features)

model_lstm = LSTMBaseline(n_features)
opt_lstm = torch.optim.Adam(model_lstm.parameters(), lr=1e-3)
loss_fn = nn.MSELoss()

# Train LSTM (quick 5 epochs)
print("\nTraining Baseline LSTM...\n")
for epoch in range(5):
    total = 0
    for xb, yb in loader:
        opt_lstm.zero_grad()
        pred = model_lstm(xb)
        loss = loss_fn(pred, yb)
        loss.backward()
        opt_lstm.step()
        total += loss.item()
    print(f"Epoch {epoch+1} Loss = {total/len(loader):.6f}")

torch.save(model_lstm.state_dict(), "lstm_baseline.pth")
print("Saved LSTM model.")

# ================================================================
# 4. SEQ2SEQ WITH BAHADANAU ATTENTION
# ================================================================
class Encoder(nn.Module):
    def __init__(self, input_size, hidden):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden, batch_first=True)

    def forward(self, x):
        output, (h, c) = self.lstm(x)
        return output, h[-1], c[-1]

class Attention(nn.Module):
    def __init__(self, hidden):
        super().__init__()
        self.W1 = nn.Linear(hidden, hidden)
        self.W2 = nn.Linear(hidden, hidden)
        self.V = nn.Linear(hidden, 1)

    def forward(self, encoder_outputs, decoder_hidden):
        seq_len = encoder_outputs.size(1)

        dh = decoder_hidden.unsqueeze(1).repeat(1, seq_len, 1)
        energy = self.V(torch.tanh(self.W1(encoder_outputs) + self.W2(dh)))
        weights = torch.softmax(energy, dim=1)
        context = torch.sum(weights * encoder_outputs, dim=1)

        return context, weights.squeeze(-1)

class Decoder(nn.Module):
    def __init__(self, n_features, hidden):
        super().__init__()
        self.attn = Attention(hidden)
        self.lstm_cell = nn.LSTMCell(n_features + hidden, hidden)
        self.fc = nn.Linear(hidden, n_features)

    def forward(self, enc_out, h, c, first_input):
        outputs = []
        attn_maps = []

        prev = first_input
        for _ in range(HORIZON):
            context, attn = self.attn(enc_out, h)
            inp = torch.cat([prev, context], dim=-1)
            h, c = self.lstm_cell(inp, (h, c))
            out = self.fc(h)

            outputs.append(out.unsqueeze(1))
            attn_maps.append(attn.unsqueeze(1))

            prev = out

        return torch.cat(outputs, 1), torch.cat(attn_maps, 1)

class Seq2Seq(nn.Module):
    def __init__(self, n_features, hidden=128):
        super().__init__()
        self.encoder = Encoder(n_features, hidden)
        self.decoder = Decoder(n_features, hidden)

    def forward(self, x):
        enc_out, h, c = self.encoder(x)
        return self.decoder(enc_out, h, c, x[:, -1, :])

model_attn = Seq2Seq(n_features)
opt_attn = torch.optim.Adam(model_attn.parameters(), lr=1e-3)

# Train Attention model
print("\nTraining Seq2Seq Attention Model...\n")
for epoch in range(5):
    total = 0
    for xb, yb in loader:
        opt_attn.zero_grad()
        pred, _ = model_attn(xb)
        loss = loss_fn(pred, yb)
        loss.backward()
        opt_attn.step()
        total += loss.item()
    print(f"Epoch {epoch+1} Loss = {total/len(loader):.6f}")

torch.save(model_attn.state_dict(), "seq2seq_attention.pth")
print("Saved Attention model.")

# ================================================================
# 5. EVALUATION (Train/Test Split)
# ================================================================
split = int(0.9 * len(X))

X_test = X[split:]
Y_test = Y[split:]

X_test_t = torch.tensor(X_test, dtype=torch.float32)

model_lstm.eval()
model_attn.eval()

with torch.no_grad():
    pred_lstm = model_lstm(X_test_t).numpy()
    pred_attn, attn_map = model_attn(X_test_t)
    pred_attn = pred_attn.numpy()
    attn_map = attn_map.numpy()

# Inverse scaling
pred_lstm_inv = scaler.inverse_transform(pred_lstm.reshape(-1, n_features)).reshape(pred_lstm.shape)
pred_attn_inv = scaler.inverse_transform(pred_attn.reshape(-1, n_features)).reshape(pred_attn.shape)

true_inv = scaler.inverse_transform(Y_test.reshape(-1, n_features)).reshape(Y_test.shape)

def metrics(true, pred):
    m1 = mean_absolute_error(true.reshape(-1, n_features), pred.reshape(-1, n_features))
    m2 = np.sqrt(mean_squared_error(true.reshape(-1, n_features), pred.reshape(-1, n_features)))
    m3 = np.mean(np.abs((true - pred) / (np.abs(true) + 1e-6))) * 100
    return m1, m2, m3

mae_l, rmse_l, mape_l = metrics(true_inv, pred_lstm_inv)
mae_a, rmse_a, mape_a = metrics(true_inv, pred_attn_inv)

print("\n=== MODEL PERFORMANCE ===")
print("LSTM → MAE:", mae_l, "RMSE:", rmse_l, "MAPE:", mape_l)
print("ATTN → MAE:", mae_a, "RMSE:", rmse_a, "MAPE:", mape_a)

# ================================================================
# 6. VISUALIZATION (Prediction + Attention Heatmap)
# ================================================================
idx = 5
feature = 0

plt.figure(figsize=(12,5))
obs = scaler.inverse_transform(X_test[idx])
future = true_inv[idx]

plt.plot(range(INPUT_LEN), obs[:, feature], label="Observed")
plt.plot(range(INPUT_LEN, INPUT_LEN+HORIZON), future[:, feature], label="True Future")
plt.plot(range(INPUT_LEN, INPUT_LEN+HORIZON), pred_lstm_inv[idx,:,feature], label="LSTM Prediction")
plt.plot(range(INPUT_LEN, INPUT_LEN+HORIZON), pred_attn_inv[idx,:,feature], label="Attention Prediction")
plt.legend()
plt.title("Forecast Comparison (Feature 0)")
plt.show()

plt.figure(figsize=(10,6))
plt.imshow(attn_map[idx], aspect='auto', origin='lower', cmap='viridis')
plt.colorbar()
plt.title("Attention Heatmap")
plt.xlabel("Encoder Time Steps")
plt.ylabel("Decoder Time Steps")
plt.show()